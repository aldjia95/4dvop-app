{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projet_Final_Récupérer_les_path_des_fichiers_images_et_effectuer_le_préprocessing (1).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aldjia95/4dvop-app/blob/master/Projet_Final_R%C3%A9cup%C3%A9rer_les_path_des_fichiers_images_et_effectuer_le_pr%C3%A9processing_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbRPFys0988n"
      },
      "source": [
        "# Importation des librairies et accès au google drive\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOjtNI2kmho6"
      },
      "source": [
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np \r\n",
        "import matplotlib.pyplot as plt \r\n",
        "\r\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PxN1oCGiK8a"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVeUfln6hwWr"
      },
      "source": [
        "## Dezipper le dossier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqq6nuUQhcjQ"
      },
      "source": [
        "zip_file = tf.keras.utils.get_file(\"raw_image.zip\", \r\n",
        "                                   \"/content/drive/MyDrive/raw_image.zip\",\r\n",
        "                                   cache_subdir=\"/content/drive/MyDrive/\", archive_format = 'zip',\r\n",
        "                                   extract=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O68CN_MFLj4w"
      },
      "source": [
        "## Les données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC5zsc6y-bhc"
      },
      "source": [
        "### Récupération des paths de mes fichiers contenant mes images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rnr04_YRQZHD"
      },
      "source": [
        "import pathlib\r\n",
        "\r\n",
        "#test folder\r\n",
        "test_folder = pathlib.Path(\"/content/drive/MyDrive/testing/\")\r\n",
        "\r\n",
        "#train foler croped images\r\n",
        "train_folder = pathlib.Path(\"/content/drive/MyDrive/training\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q02aDelZCBk2"
      },
      "source": [
        "#### Récupération des chemins de chaque image (en format str)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ypTOpdbSgEF"
      },
      "source": [
        "# test --> Je transforme également mes images au format jpg.\r\n",
        "all_image_paths_for_test = [str(img_path) for img_path in list(test_folder.glob(\"*.jpg\"))]\r\n",
        "\r\n",
        "#train --> Je transforme également mes images au format jpg.\r\n",
        "all_image_paths_for_train = [str(img_path) for img_path in list(train_folder.glob(\"*.jpg\"))]\r\n",
        "\r\n",
        "\r\n",
        "display(all_image_paths_for_test[-5:])\r\n",
        "print('\\n')\r\n",
        "display(all_image_paths_for_train[-5:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnKYq3uFBM0K"
      },
      "source": [
        "### Déterminer le nombre d'image de mes fichiers à l'aide du nombre des paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNvwHHvNBMV-"
      },
      "source": [
        "print('Number of images in Train :')\r\n",
        "print(len(all_image_paths_for_train))\r\n",
        "print('________________________________________________')\r\n",
        "print('Number of images in Test :')\r\n",
        "print(len(all_image_paths_for_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5yd7KvylWuM"
      },
      "source": [
        "# Creation des labels \r\n",
        "\r\n",
        "Nous voulons ici récupérer les labels à partir des titres des images. Nous avons un exemple : ---> '9_img_1553497170231.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljr-0lfmUZfL"
      },
      "source": [
        "get_fichier = lambda f: os.path.split(f)[-1]\r\n",
        "get_labels =lambda f: [f[0]]\r\n",
        "\r\n",
        "\r\n",
        "process = lambda f: get_labels(get_fichier(f))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bOWTlxyUdtr"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "label_test=[]\r\n",
        "for paths in all_image_paths_for_test:\r\n",
        "    a = process(paths)\r\n",
        "    label_test += a\r\n",
        "\r\n",
        "label_train=[]\r\n",
        "for paths in all_image_paths_for_train:\r\n",
        "    a = process(paths)\r\n",
        "    label_train += a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Vb_TdgNbF--"
      },
      "source": [
        "\r\n",
        "\r\n",
        "label_test = [int(x) for x in label_test]\r\n",
        "\r\n",
        "label_train = [int(x) for x in label_train]\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbtWLDMPnG9E"
      },
      "source": [
        "### création de nos variables de validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZy8IBJRo4Az"
      },
      "source": [
        "* Train = 45000 images\r\n",
        "\r\n",
        "* Test = 5000 images\r\n",
        "\r\n",
        "* Val = 5000 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hEOG2rQnbJB"
      },
      "source": [
        "# Récupération des paths et des labels d'entraînement que j'associe dans X et Y : \r\n",
        "X = all_image_paths_for_train\r\n",
        "Y = label_train\r\n",
        "\r\n",
        "# Récupération des paths et des labels de test que j'associe dans X_test et Y_test : \r\n",
        "X_test = all_image_paths_for_test\r\n",
        "Y_test = label_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOFaJwfnnVzR"
      },
      "source": [
        "# Je récupère 10% des paths de train que j'implémente dans X_val et Y_val : \r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=1,shuffle=True) # = 5000 images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8ROzH21pDJb"
      },
      "source": [
        "len(X_train)\r\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8A5-v3ddFDi"
      },
      "source": [
        "### Creation de mes tenseurs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdsr45Jhpa9l"
      },
      "source": [
        "tensortrain_set = tf.data.Dataset.from_tensor_slices((X_train))\r\n",
        "tensortval_set = tf.data.Dataset.from_tensor_slices((X_val))\r\n",
        "tensortest_set = tf.data.Dataset.from_tensor_slices((X_test))\r\n",
        "\r\n",
        "Y_train = tf.data.Dataset.from_tensor_slices(Y_train)\r\n",
        "Y_val = tf.data.Dataset.from_tensor_slices(Y_val)  \r\n",
        "Y_test= tf.data.Dataset.from_tensor_slices(Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQEGxqvMEMtw"
      },
      "source": [
        "### Fonction pour transformer mes paths en image et data-augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5wgA7H7pjC9"
      },
      "source": [
        "def data_aug(img, label):\r\n",
        "\r\n",
        "  img= tf.io.read_file(img) # Lire mes paths en image\r\n",
        "  img = tf.image.decode_jpeg(img, channels=3) # Transformer mes images au format jpeg\r\n",
        "  img = tf.image.resize(img, [64, 64]) # Mettre mes images à la même échelle\r\n",
        "  img = tf.image.random_flip_left_right(img) # Rotation de mes images\r\n",
        "  img = tf.image.random_contrast(img, 0.9, 1.2) # Random contrast \r\n",
        "  img = tf.image.random_crop(img, [64,64,3]) # Zoom random des images\r\n",
        "  img = img / 255 # Normalisation de mes pixels\r\n",
        "\r\n",
        "  return img, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTtGSWBAsQev"
      },
      "source": [
        "tensortrain_set = tf.data.Dataset.zip((tensortrain_set, Y_train))\r\n",
        "train_set = tensortrain_set.repeat(3) # Je multiplie par 3 mes images d'entraînement --> Data-augmentation\r\n",
        "train_set = train_set.map(data_aug) # J'applique la fonction data_aug dans mes données d'entrainement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF6S5jxAsaQW"
      },
      "source": [
        "### Transformation des données de test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC964Zh4sXLi"
      },
      "source": [
        "def load_and_preprocess_images(img):\r\n",
        "  img = tf.io.read_file(img)\r\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\r\n",
        "  img = tf.image.resize(img, [64, 64])\r\n",
        "  img = img / 255\r\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FARsZQlXseA8"
      },
      "source": [
        "tensortval_set = tensortval_set.map(load_and_preprocess_images) # J'applique la fonction load_and_preprocess_images dans mes données de validation\r\n",
        "val_set = tf.data.Dataset.zip((tensortval_set, Y_val))\r\n",
        "\r\n",
        "tensortest_set = tensortest_set.map(load_and_preprocess_images) # J'applique la fonction load_and_preprocess_images dans mes données de de test\r\n",
        "test_set = tf.data.Dataset.zip((tensortest_set, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkwMN1zpsjz5"
      },
      "source": [
        "print('size of repeated_train_set is {} total images'.format(len(train_set)))\r\n",
        "print('size of val_set is {} images total'.format(len(val_set)))\r\n",
        "print('size of test_set is {} images total'.format(len(test_set)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaQgB46xsmTP"
      },
      "source": [
        "train_set = train_set.shuffle(len(train_set)).batch(10)\r\n",
        "\r\n",
        "val_set = val_set.batch(10)\r\n",
        "\r\n",
        "test_set = test_set.batch(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcK8bScVIzYv"
      },
      "source": [
        "for example_x, example_y in train_set.take(1):\r\n",
        "  plt.figure()\r\n",
        "  plt.title(example_y[0].numpy())\r\n",
        "  plt.imshow(example_x[0].numpy())\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGTLBb3yqK2W"
      },
      "source": [
        "for x in Y_train:\r\n",
        "  print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFXFfmNOjiea"
      },
      "source": [
        "for x,y in train_set.take(1):\r\n",
        "  print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNciFKsjKUA-"
      },
      "source": [
        "for example_x, example_y in train_set.take(1):\r\n",
        "  plt.figure()\r\n",
        "  plt.title(example_y[0].numpy())\r\n",
        "  plt.imshow(example_x[0].numpy())\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmAbq4RX2IW_"
      },
      "source": [
        "## 3 - Deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6iURCG62cqB"
      },
      "source": [
        "Now, it's time to build your neural network.\r\n",
        "* Create a CNN model in which you will put 2D convolutional layers and MaxPool2D layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppp8pG2jBuUa"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[60, 60, 3]),\r\n",
        "    tf.keras.layers.MaxPool2D(pool_size=21, strides=2, padding='valid'),\r\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"),\r\n",
        "    tf.keras.layers.MaxPool2D(pool_size=10, strides=2, padding='valid'),\r\n",
        "    tf.keras.layers.Flatten(),\r\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\r\n",
        "    tf.keras.layers.Dropout(0.05),\r\n",
        "    tf.keras.layers.Dense(units=32, activation =\"relu\"),\r\n",
        "    tf.keras.layers.Dense(units=16, activation =\"relu\"),\r\n",
        "    tf.keras.layers.Dense(units=10, activation='softmax')\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikqiIVkM7um5"
      },
      "source": [
        "* Bonus : Create a _Learning Rate Schedule_. You can choose to do this with [_ExponentialDecay_](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) (with learning_rate = 0.0005)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3o_AmZXBzZh"
      },
      "source": [
        "#  Let's create a learning rate schedule to decrease the learning rate as we train the model. \r\n",
        "initial_learning_rate = 0.0005\r\n",
        "\r\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\r\n",
        "    initial_learning_rate,\r\n",
        "    decay_steps=6000,\r\n",
        "    decay_rate=0.95,\r\n",
        "    staircase=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIkvOaYY73FB"
      },
      "source": [
        "* Create a compiler in which you choose : \r\n",
        "  * Your optimizer (with learning_rate = 0.0005)\r\n",
        "  * Your loss \r\n",
        "  * Your metrics "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uPJH3o_CP9P"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\r\n",
        "              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\r\n",
        "              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2Y7MVSr7_6S"
      },
      "source": [
        "* Put the model in a variable called \"history\" and run on 10 epochs. Use `validation_data`argument for your validation data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op5AWF4vB7uw"
      },
      "source": [
        "history = model.fit(X_train, validation_data = X_test, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKzTo9rQ5oK9"
      },
      "source": [
        "* Evaluate the model with test data.\r\n",
        "\r\n",
        "What is the conclusion about these results ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aHMsu8vC4zB"
      },
      "source": [
        "model.evaluate(ds_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tw44wnJ2wEX"
      },
      "source": [
        "* Now watch a graph of how your *loss* evolves as epochs advance.\r\n",
        "  * For that, you can use the evolution of your model with `.history` function with \"loss\", \"val_loss\" for loss function and \"sparse_categorical_accuracy\", \"val_sparse_categorical_accuracy\" for evolution of accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WArS917D2T0"
      },
      "source": [
        "plt.plot(history.history[\"loss\"], color=\"b\")\r\n",
        "plt.plot(history.history[\"val_loss\"], color=\"r\")\r\n",
        "plt.ylabel(\"loss\")\r\n",
        "plt.xlabel(\"Epochs\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0C0FgAfD2Lo"
      },
      "source": [
        "plt.plot(history.history[\"sparse_categorical_accuracy\"], color=\"b\")\r\n",
        "plt.plot(history.history[\"val_sparse_categorical_accuracy\"], color=\"r\")\r\n",
        "plt.ylabel(\"sparse_categorical_accuracy\")\r\n",
        "plt.xlabel(\"Epochs\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0e7Zr9ZWAeE"
      },
      "source": [
        "* Make a **confusion matrix** that determines the predictions between each class (True positive, False positive...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyzW2hZFWCUP"
      },
      "source": [
        "predict = model.predict(ds_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L-uDxeMWTb3"
      },
      "source": [
        "import seaborn as sns\r\n",
        "figure = plt.figure(figsize=(10, 8))\r\n",
        "sns.heatmap(tf.math.confusion_matrix(Y_test,tf.argmax(predict, axis=1).numpy()), annot=True, cmap=plt.cm.Reds)\r\n",
        "plt.tight_layout()\r\n",
        "plt.ylabel('True label')\r\n",
        "plt.xlabel('Predicted label')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6AZhOhAWVsS"
      },
      "source": [
        "What's your conclusion ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKbyeiM9Q2UC"
      },
      "source": [
        "## 4 - Save and load your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS_rTCTOQURy"
      },
      "source": [
        "* Save your model in drive. For that, import and use the class [ModelCheckpoint](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqCBchORQZmj"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\r\n",
        "checkpoint = ModelCheckpoint(\"/content/Dossier modèle/Mon_modèle_projet_mnist.h5\",\r\n",
        "                             monitor='val_sparse_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2RE96qLRgl9"
      },
      "source": [
        "history = model.fit(ds_train, validation_data = ds_valid, epochs=10, callbacks=[checkpoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q_KvprMQ0zt"
      },
      "source": [
        "* Load your model in your code with [tf.keras.models.load_model](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ErPGyJHReXZ"
      },
      "source": [
        "model = tf.keras.models.load_model(\"/content/Dossier modèle/Mon_modèle_projet_mnist.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEPxFNf6ELHR"
      },
      "source": [
        "## What's your conclusion ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAzZTun-8UMj"
      },
      "source": [
        ""
      ]
    }
  ]
}